# Fichier : streamlit_app/utils.py
# Fonctions utilitaires pour l'application Streamlit
# ====================================================
#
# OBJECTIF PRINCIPAL :
# Ce fichier contient toutes les fonctions r√©utilisables de l'interface Streamlit.
# Il s√©pare la logique m√©tier (utils) de l'interface utilisateur (app.py).
#
# PRINCIPE DE CONCEPTION :
# "Separation of Concerns" - Chaque fonction a UNE responsabilit√© claire :
# - Chargement des mod√®les
# - Pr√©dictions
# - Acc√®s aux donn√©es
# - Validation des inputs
# - Formatage de l'affichage
#
# AVANTAGES DE CETTE ARCHITECTURE :
# ‚úÖ Code r√©utilisable (fonctions appel√©es partout dans l'app)
# ‚úÖ Tests faciles (chaque fonction testable ind√©pendamment)
# ‚úÖ Maintenance simple (bug ? chercher dans la fonction concern√©e)
# ‚úÖ Performances (cache Streamlit pour √©viter rechargements inutiles)
#
# STRUCTURE DU FICHIER :
# 1. Chargement des mod√®les (avec cache)
# 2. Fonctions de pr√©diction
# 3. Fonctions d'acc√®s aux donn√©es
# 4. Fonctions d'affichage et validation
# 5. Utilitaires divers

import streamlit as st
import pandas as pd
import numpy as np
import joblib
import tensorflow as tf
from pathlib import Path
import sys
from datetime import datetime

# === CONFIGURATION DES CHEMINS ===
# Ajouter le dossier parent au path pour importer src/
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.config import (
    LGBM_MODEL_FILE, LSTM_MODEL_FILE, SCALER_FILE, ENCODERS_FILE,
    QUARTIER_COORDS, THRESHOLD_MODERATE, THRESHOLD_HIGH,
    SEQUENCE_LENGTH
)
from src.data_pipeline import DataPipeline
from src.database import DatabaseManager


# ============================================================================
# SECTION 1 : CHARGEMENT DES MOD√àLES (AVEC CACHE)
# ============================================================================

@st.cache_resource
def load_models():
    """
    Charge les mod√®les ML, scaler et encodeur (UNE SEULE FOIS).
    
    CACHE STREAMLIT (@st.cache_resource) :
    Cette d√©coration est CRUCIALE pour les performances !
    
    Sans cache :
    - Les mod√®les se rechargent √† CHAQUE interaction (slider, bouton)
    - Temps de chargement : ~5 secondes par interaction
    - Utilisateur frustr√© ‚Üí application inutilisable
    
    Avec cache :
    - Chargement UNE FOIS au d√©marrage
    - Interactions instantan√©es ensuite
    - Les objets sont partag√©s entre toutes les sessions utilisateurs
    
    DIFF√âRENCE @cache_resource vs @cache_data :
    - @cache_resource : Pour objets NON-s√©rialisables (mod√®les, connexions DB)
    - @cache_data : Pour donn√©es s√©rialisables (DataFrames, listes, dict)
    
    QUAND LE CACHE EST VID√â :
    - Red√©marrage de l'application
    - Modification du code de cette fonction
    - Bouton "Clear cache" dans l'interface Streamlit
    
    Returns:
        tuple: (lgbm_model, lgbm_threshold, lstm_model, lstm_threshold, 
                scaler, label_encoder) ou (None, ..., None) si erreur
    """
    try:
        # === CHARGEMENT LIGHTGBM ===
        # LightGBM est sauvegard√© avec Joblib (format pickle optimis√©)
        lgbm_data = joblib.load(LGBM_MODEL_FILE)
        lgbm_model = lgbm_data['model']      # Le mod√®le entra√Æn√©
        lgbm_threshold = lgbm_data['threshold']  # Seuil optimal (ex: 0.21)
        
        # === CHARGEMENT LSTM ===
        # LSTM est sauvegard√© avec Keras (format HDF5)
        # compile=False : Pas besoin de recompiler (on fait juste des pr√©dictions)
        lstm_model = tf.keras.models.load_model(LSTM_MODEL_FILE, compile=False)
        
        # Le seuil LSTM est dans un fichier texte s√©par√©
        lstm_threshold_file = LSTM_MODEL_FILE.parent / "lstm_threshold.txt"
        with open(lstm_threshold_file, 'r') as f:
            lstm_threshold = float(f.read().strip())
        
        # === CHARGEMENT SCALER ===
        # StandardScaler pour normaliser les features (m√™me √©chelle que l'entra√Ænement)
        scaler = joblib.load(SCALER_FILE)
        
        # === CHARGEMENT ENCODEUR ===
        # LabelEncoder pour transformer les quartiers en nombres
        encoders = joblib.load(ENCODERS_FILE)
        label_encoder = encoders['quartier']
        
        return lgbm_model, lgbm_threshold, lstm_model, lstm_threshold, scaler, label_encoder
    
    except Exception as e:
        # Afficher l'erreur dans l'interface Streamlit (zone rouge)
        st.error(f"‚ùå Erreur lors du chargement des mod√®les : {e}")
        return None, None, None, None, None, None


@st.cache_resource
def get_database():
    """
    Initialise la connexion √† la base de donn√©es (UNE SEULE FOIS).
    
    POURQUOI CACHER LA CONNEXION DB ?
    - Ouvrir/fermer une connexion DB √† chaque requ√™te est LENT
    - Le cache maintient la connexion ouverte pendant toute la session
    - Pool de connexions partag√© entre utilisateurs
    
    GESTION D'ERREUR :
    Si la BD n'est pas disponible (fichier manquant, corruption), 
    l'application continue de fonctionner SANS historique.
    
    ALTERNATIVE SI BD INDISPONIBLE :
    - Pr√©dictions en temps r√©el : ‚úÖ Fonctionnent
    - Historique : ‚ùå Non disponible
    - Statistiques : ‚ùå Non disponibles
    
    Returns:
        DatabaseManager: Instance de connexion √† la BD ou None si erreur
    """
    try:
        db = DatabaseManager()
        if db.connect():
            return db
        return None
    except Exception as e:
        # Warning (jaune) au lieu d'error (rouge) car l'app peut fonctionner sans BD
        st.warning(f"‚ö†Ô∏è Base de donn√©es non disponible : {e}")
        return None


# ============================================================================
# SECTION 2 : FONCTIONS DE PR√âDICTION
# ============================================================================

def make_prediction_single(input_data, quartier, lgbm_model, lgbm_threshold, 
                             lstm_model, lstm_threshold, scaler, label_encoder,
                             historical_data=None):
    """
    Effectue une pr√©diction pour une seule entr√©e utilisateur.
    
    WORKFLOW DE PR√âDICTION :
    1. Cr√©er un DataFrame avec les donn√©es d'entr√©e
    2. G√©n√©rer les features temporelles (heure, jour, mois, etc.)
    3. Encoder le quartier (texte ‚Üí nombre)
    4. Normaliser les features (StandardScaler)
    5. Pr√©diction LightGBM (toujours disponible)
    6. Pr√©diction LSTM (si historique disponible)
    7. Calculer la probabilit√© moyenne
    8. D√©terminer le statut de risque (Faible/Mod√©r√©/√âlev√©)
    
    POURQUOI DEUX MOD√àLES ?
    - LightGBM : Rapide, pr√©cis, fonctionne TOUJOURS
    - LSTM : Capture les tendances temporelles, n√©cessite historique
    
    La moyenne des deux donne une pr√©diction plus robuste (ensemble learning).
    
    GESTION DES CAS LIMITES :
    - Quartier inconnu : Utilise la premi√®re classe connue (pas de crash)
    - Historique insuffisant : Utilise seulement LightGBM
    - Erreur LSTM : Utilise seulement LightGBM (graceful degradation)
    
    Args:
        input_data (dict): Donn√©es saisies par l'utilisateur
            {
                'temperature': float,   # ¬∞C
                'humidite': float,      # %
                'vent': float,          # km/h
                'consommation': float   # MW
            }
        quartier (str): Nom du quartier (ex: "Dakar-Plateau")
        lgbm_model: Mod√®le LightGBM charg√©
        lgbm_threshold (float): Seuil de d√©cision LightGBM (ex: 0.21)
        lstm_model: Mod√®le LSTM charg√©
        lstm_threshold (float): Seuil de d√©cision LSTM (ex: 0.50)
        scaler: StandardScaler pour normalisation
        label_encoder: LabelEncoder pour les quartiers
        historical_data (pd.DataFrame, optional): Donn√©es historiques pour LSTM
    
    Returns:
        dict: R√©sultats complets de pr√©diction
            {
                'proba_lgbm': float,        # Probabilit√© LightGBM (0.0-1.0)
                'pred_lgbm': int,           # Pr√©diction LightGBM (0 ou 1)
                'proba_lstm': float,        # Probabilit√© LSTM (0.0-1.0)
                'pred_lstm': int,           # Pr√©diction LSTM (0 ou 1)
                'proba_moyenne': float,     # Moyenne des probabilit√©s
                'statut': str,              # "Risque Faible/Mod√©r√©/√âlev√©"
                'color': str,               # "green/orange/red"
                'emoji': str,               # "üü¢/üü†/üî¥"
                'seuil_lgbm': float,        # Seuil utilis√©
                'seuil_lstm': float,        # Seuil utilis√©
                'lstm_utilisable': bool     # LSTM a pu pr√©dire ?
            }
    """
    
    # 1. Cr√©er le DataFrame d'entr√©e
    df_input = pd.DataFrame([{
        'temp_celsius': input_data['temperature'],
        'humidite_percent': input_data['humidite'],
        'vitesse_vent': input_data['vent'],
        'conso_megawatt': input_data['consommation'],
        'date_heure': pd.Timestamp.now(),
        'quartier': quartier
    }])
    
    # 2. Cr√©er les features temporelles
    pipeline = DataPipeline()
    df_input = pipeline.create_time_features(df_input)
    
    # 3. Encoder le quartier avec logique de secours (coh√©rence en production)
    try:
        if hasattr(label_encoder, 'classes_') and quartier in label_encoder.classes_:
            # Quartier connu
            df_input['quartier_encoded'] = label_encoder.transform([quartier])[0]
        else:
            # Quartier inconnu: utilise la premi√®re classe connue ou 0 par d√©faut
            if hasattr(label_encoder, 'classes_') and len(label_encoder.classes_) > 0:
                # Utilise la premi√®re classe pour √©viter une erreur de LabelEncoder sur une nouvelle classe
                df_input['quartier_encoded'] = label_encoder.transform([label_encoder.classes_[0]])[0] 
            else:
                df_input['quartier_encoded'] = 0
    except Exception as e:
        df_input['quartier_encoded'] = 0
        print(f"‚ö†Ô∏è Erreur encodage quartier {quartier}: {e}. Utilisation valeur par d√©faut: 0")
    
    # 4. Pr√©parer les features (9 colonnes)
    feature_cols = [
        'temp_celsius', 'humidite_percent', 'vitesse_vent', 'conso_megawatt',
        'heure', 'jour_semaine', 'mois', 'is_peak_hour', 'quartier_encoded'
    ]
    
    X_input = df_input[feature_cols].values
    
    # 5. Normaliser l'input (n√©cessaire pour LSTM et utilis√© pour LGBM pour la coh√©rence du pipeline)
    X_scaled = scaler.transform(X_input)
    
    # 6. Pr√©diction LightGBM 
    # CORRECTION : Le mod√®le LightGBM (Booster) n'a pas de predict_proba().
    # On utilise predict(raw_score=True) pour obtenir le logit, puis on applique la sigmo√Øde.
    try:
        # Obtenir le logit (score brut)
        logit_lgbm = lgbm_model.predict(X_scaled, raw_score=True)[0]
        # Appliquer la fonction sigmo√Øde pour obtenir la probabilit√© P(Y=1)
        proba_lgbm = 1 / (1 + np.exp(-logit_lgbm))
    except Exception as e:
        # Logique de secours si raw_score n'est pas support√© ou autre erreur inattendue.
        print(f"‚ö†Ô∏è Erreur: Incapacit√© de calculer la probabilit√© LightGBM. Forcer proba √† 0.05. Erreur: {e}")
        proba_lgbm = 0.05
        

    pred_lgbm = 1 if proba_lgbm >= lgbm_threshold else 0
    
    # 7. Pr√©diction LSTM
    proba_lstm = None
    pred_lstm = 0
    
    if historical_data is not None and len(historical_data) >= SEQUENCE_LENGTH - 1:
        try:
            # Pr√©parer la s√©quence historique
            df_hist = historical_data.copy()
            df_hist = pipeline.create_time_features(df_hist)
            
            # Encoder les quartiers de l'historique (logique robuste)
            try:
                df_hist['quartier_encoded'] = df_hist['quartier'].apply(
                    lambda x: label_encoder.transform([x])[0] 
                    if hasattr(label_encoder, 'classes_') and x in label_encoder.classes_ 
                    else 0
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur encodage quartier historique: {e}. Utilisation valeur par d√©faut: 0")
                df_hist['quartier_encoded'] = 0 
            
            # Concat√©ner historique + nouvelle entr√©e
            df_sequence = pd.concat([
                df_hist.tail(SEQUENCE_LENGTH - 1)[feature_cols], 
                df_input[feature_cols]
            ])
            X_seq = df_sequence.values
            
            # Mettre √† l'√©chelle la s√©quence (OBLIGATOIRE pour LSTM)
            X_seq_scaled = scaler.transform(X_seq)
            
            # Reshaper pour LSTM (samples, timesteps, features)
            X_seq_scaled = X_seq_scaled.reshape(1, SEQUENCE_LENGTH, len(feature_cols))
            
            # Pr√©diction LSTM
            proba_lstm = lstm_model.predict(X_seq_scaled, verbose=0)[0][0]
            pred_lstm = 1 if proba_lstm >= lstm_threshold else 0
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur pr√©diction LSTM: {e}. Utilisation uniquement de LightGBM.")
            proba_lstm = None
    
    # 8. Calculer la probabilit√© moyenne
    valid_probas = [proba_lgbm]
    if proba_lstm is not None:
        valid_probas.append(proba_lstm)
    
    # S'assurer qu'il y a des probabilit√©s valides
    if not valid_probas:
        proba_moyenne = 0.0
    else:
        proba_moyenne = sum(valid_probas) / len(valid_probas)
    
    # Valeur d'affichage pour LSTM
    display_proba_lstm = proba_lstm if proba_lstm is not None else 0.0

    # 9. D√©terminer le statut de risque
    if proba_moyenne >= THRESHOLD_HIGH:
        statut = "Risque √âlev√©"
        color = "red"
        emoji = "üî¥"
    elif proba_moyenne >= THRESHOLD_MODERATE:
        statut = "Risque Mod√©r√©"
        color = "orange"
        emoji = "üü†"
    else:
        statut = "Risque Faible"
        color = "green"
        emoji = "üü¢"
    
    return {
        'proba_lgbm': proba_lgbm,
        'pred_lgbm': pred_lgbm,
        'proba_lstm': display_proba_lstm,
        'pred_lstm': pred_lstm,
        'proba_moyenne': proba_moyenne,
        'statut': statut,
        'color': color,
        'emoji': emoji,
        'seuil_lgbm': lgbm_threshold,
        'seuil_lstm': lstm_threshold,
        'lstm_utilisable': proba_lstm is not None
    }


# ============================================================================
# SECTION 3 : FONCTIONS D'ACC√àS AUX DONN√âES
# ============================================================================

def get_historical_data(db, quartier=None, hours=168):
    """
    R√©cup√®re les donn√©es historiques depuis la BD
    
    USAGE TYPIQUE :
    - Afficher l'historique des coupures (graphiques)
    - Fournir des donn√©es pour LSTM (besoin de s√©quence temporelle)
    - Calculer des statistiques (taux de coupures r√©cent)
    
    PARAM√àTRE hours=168 :
    168 heures = 7 jours (1 semaine d'historique par d√©faut)
    
    STRAT√âGIE DE R√âCUP√âRATION :
    On demande hours * 2 enregistrements pour avoir une marge de s√©curit√©.
    Pourquoi ? La BD peut avoir des trous (heures manquantes).
    
    Args:
        db (DatabaseManager): Instance de la BD
        quartier (str): Filtrer par quartier (optionnel)
        hours (int): Nombre d'heures √† r√©cup√©rer
        
    Returns:
        pd.DataFrame: Donn√©es historiques
    """
    if db is None:
        return pd.DataFrame()
    
    try:
        # R√©cup√©rer plus de points que n√©cessaire pour s'assurer d'avoir la s√©quence compl√®te
        df = db.get_enregistrements(quartier=quartier, limit=hours * 2) 
        
        if not df.empty:
            df['date_heure'] = pd.to_datetime(df['date_heure'])
            df = df.sort_values('date_heure')
        
        return df
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur r√©cup√©ration donn√©es historiques: {e}")
        return pd.DataFrame()


def get_statistics_by_quartier(db):
    """
    R√©cup√®re les statistiques par quartier
    
    M√âTRIQUES CALCUL√âES :
    - total_enregistrements : Nombre d'observations
    - total_coupures : Nombre de coupures d√©tect√©es
    - taux_coupure : % de coupures (0-100)
    - temp_moyenne : Temp√©rature moyenne (¬∞C)
    - conso_moyenne : Consommation moyenne (MW)
    
    USAGE :
    - Dashboard r√©capitulatif
    - Comparaison entre quartiers
    - Identification des zones √† risque
    
    Args:
        db (DatabaseManager): Instance de la BD
        
    Returns:
        pd.DataFrame: Statistiques agr√©g√©es par quartier
    """
    if db is None:
        return pd.DataFrame()
    
    try:
        query = """
        SELECT 
            quartier,
            COUNT(*) as total_enregistrements,
            SUM(coupure) as total_coupures,
            AVG(coupure) * 100 as taux_coupure,
            AVG(temp_celsius) as temp_moyenne,
            AVG(conso_megawatt) as conso_moyenne
        FROM enregistrements
        GROUP BY quartier
        ORDER BY taux_coupure DESC
        """
        
        df = pd.read_sql(query, db.engine)
        return df
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur r√©cup√©ration statistiques: {e}")
        return pd.DataFrame()


# ============================================================================
# SECTION 4 : FONCTIONS D'AFFICHAGE ET VALIDATION
# ============================================================================

def format_percentage(value):
    """
    Formate un nombre en pourcentage
    
    Exemple:
        0.07234 ‚Üí "7.23%"
        0.9 ‚Üí "90.00%"
    
    Args:
        value (float): Nombre entre 0 et 1
    
    Returns:
        str: Pourcentage format√©
    """
    return f"{value * 100:.2f}%"


def get_risk_color(probability):
    """
    Retourne la couleur selon le niveau de risque
    
    MAPPING :
    - [0.7, 1.0] ‚Üí "red" (Risque √âlev√©)
    - [0.3, 0.7[ ‚Üí "orange" (Risque Mod√©r√©)
    - [0.0, 0.3[ ‚Üí "green" (Risque Faible)
    
    Args:
        probability (float): Probabilit√© de coupure (0.0-1.0)
    
    Returns:
        str: "red", "orange" ou "green"
    """
    if probability >= THRESHOLD_HIGH:
        return "red"
    elif probability >= THRESHOLD_MODERATE:
        return "orange"
    else:
        return "green"


def display_metric_card(label, value, delta=None, help_text=None):
    """
    Affiche une m√©trique stylis√©e
    
    COMPOSANT STREAMLIT st.metric :
    Affiche une carte avec :
    - Label (titre)
    - Value (valeur principale)
    - Delta (variation, optionnel)
    - Help text (info-bulle, optionnel)
    
    Args:
        label (str): Titre de la m√©trique
        value: Valeur √† afficher (peut √™tre str, int, float)
        delta: Variation par rapport √† une r√©f√©rence (optionnel)
        help_text (str): Texte d'aide au survol (optionnel)
    """
    st.metric(label=label, value=value, delta=delta, help=help_text)


def validate_input(temperature, humidite, vent, consommation):
    """
    Valide les entr√©es utilisateur
    
    RANGES DE VALIDATION :
    - Temp√©rature : 15-40¬∞C (climat de Dakar)
    - Humidit√© : 30-100% (physiquement possible)
    - Vent : 0-50 km/h (vents normaux √† cycloniques)
    - Consommation : 200-1500 MW (capacit√© du r√©seau de Dakar)
    
    Args:
        temperature (float): Temp√©rature en ¬∞C
        humidite (float): Humidit√© en %
        vent (float): Vitesse du vent en km/h
        consommation (float): Consommation en MW
    
    Returns:
        tuple: (is_valid, error_message)
    """
    errors = []
    
    if not (15 <= temperature <= 40):
        errors.append("‚ùå Temp√©rature doit √™tre entre 15¬∞C et 40¬∞C")
    
    if not (30 <= humidite <= 100):
        errors.append("‚ùå Humidit√© doit √™tre entre 30% et 100%")
    
    if not (0 <= vent <= 50):
        errors.append("‚ùå Vitesse du vent doit √™tre entre 0 et 50 km/h")
    
    if not (200 <= consommation <= 1500):
        errors.append("‚ùå Consommation doit √™tre entre 200 et 1500 MW")
    
    if errors:
        return False, "\n".join(errors)
    
    return True, ""


# ============================================================================
# SECTION 5 : UTILITAIRES DIVERS
# ============================================================================

def get_quartier_coords():
    """
    Retourne les coordonn√©es des quartiers
    
    SOURCE : config.py ‚Üí QUARTIER_COORDS
    
    FORMAT :
    {
        'Dakar-Plateau': {'lat': 14.6937, 'lon': -17.4441},
        'Gu√©diawaye': {'lat': 14.7692, 'lon': -17.3862},
        ...
    }
    
    USAGE :
    Afficher les quartiers sur une carte Streamlit
    
    Returns:
        dict: Coordonn√©es GPS par quartier
    """
    return QUARTIER_COORDS


def get_quartier_list():
    """
    Retourne la liste des quartiers
    
    USAGE :
    - Populate un selectbox Streamlit
    - Valider un nom de quartier
    
    Returns:
        list: Liste des noms de quartiers
    """
    return list(QUARTIER_COORDS.keys())


def save_prediction_to_db(db, prediction_data):
    """
    Sauvegarde une pr√©diction dans la BD
    
    POURQUOI SAUVEGARDER LES PR√âDICTIONS ?
    - Tra√ßabilit√© : Historique des pr√©dictions faites
    - Analyse : Comparer pr√©dictions vs r√©alit√©
    - Monitoring : D√©tecter les d√©rives du mod√®le
    - Audit : Qui a pr√©dit quoi et quand ?
    
    DONN√âES SAUVEGARD√âES :
    - Date/heure de la pr√©diction
    - Quartier concern√©
    - Features d'entr√©e (temp, humidit√©, vent, conso)
    - Probabilit√©s pr√©dites (LightGBM, LSTM, moyenne)
    - Statut de risque (Faible/Mod√©r√©/√âlev√©)
    
    Args:
        db (DatabaseManager): Instance de la BD
        prediction_data (dict): Donn√©es de pr√©diction
        
    Returns:
        int: ID de la pr√©diction (ou None si erreur)
    """
    if db is None:
        return None
    
    try:
        pred_id = db.insert_prediction(prediction_data)
        return pred_id
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur sauvegarde pr√©diction en BD: {e}")
        return None


# ============================================================================
# NOTES P√âDAGOGIQUES POUR DATA SCIENTIST JUNIOR
# ============================================================================

"""
üìö CONCEPTS CL√âS √Ä RETENIR :

1. ARCHITECTURE MODULAIRE (SEPARATION OF CONCERNS)
   ------------------------------------------------
   Ce fichier utils.py s√©pare la LOGIQUE M√âTIER de l'INTERFACE UTILISATEUR.
   
   Principe :
   ‚ùå MAUVAIS : Tout dans app.py (1000+ lignes, illisible)
   ‚úÖ BON : Logique dans utils.py, UI dans app.py
   
   Avantages :
   - Code r√©utilisable (fonctions appel√©es partout)
   - Tests faciles (chaque fonction testable ind√©pendamment)
   - Maintenance simple (1 bug = 1 fonction √† corriger)
   - Collaboration facilit√©e (plusieurs d√©veloppeurs)

2. CACHE STREAMLIT (@st.cache_resource et @st.cache_data)
   -------------------------------------------------------
   Le cache est ESSENTIEL pour les performances de Streamlit.
   
   Sans cache : Chaque interaction (clic, slider) RECHARGE TOUT
   Avec cache : Chargement UNE FOIS, puis r√©utilisation
   
   Deux types de cache :
   
   @st.cache_resource ‚Üí Pour objets NON-s√©rialisables
   - Mod√®les ML (LightGBM, LSTM)
   - Connexions BD
   - Sessions r√©seau
   
   @st.cache_data ‚Üí Pour donn√©es s√©rialisables
   - DataFrames
   - Listes, dictionnaires
   - R√©sultats de calculs

3. GESTION ROBUSTE DES ERREURS (GRACEFUL DEGRADATION)
   ---------------------------------------------------
   Une bonne application ne crash JAMAIS pour l'utilisateur.
   
   Principe : Si quelque chose √©choue, l'app continue avec fonctionnalit√©s r√©duites.
   
   Exemples dans ce fichier :
   - BD inaccessible ? ‚Üí Pr√©dictions temps r√©el fonctionnent toujours
   - LSTM √©choue ? ‚Üí Utilise seulement LightGBM
   - Quartier inconnu ? ‚Üí Utilise une valeur par d√©faut

4. PR√âDICTIONS ENSEMBLE (LIGHTGBM + LSTM)
   ----------------------------------------
   On utilise DEUX mod√®les pour plus de robustesse.
   
   LightGBM :
   - Rapide (millisecondes)
   - Fonctionne toujours (pas besoin d'historique)
   - Excellent sur donn√©es tabulaires
   
   LSTM :
   - Capture les tendances temporelles
   - N√©cessite historique (12 heures)
   - Plus lent (quelques secondes)
   
   Pr√©diction finale = MOYENNE des deux

5. NORMALISATION DES FEATURES (STANDARDSCALER)
   --------------------------------------------
   CRITIQUE : Les features doivent avoir la m√™me √©chelle qu'√† l'entra√Ænement.
   
   StandardScaler transforme : X_scaled = (X - mean) / std
   
   ‚ö†Ô∏è ATTENTION : Utiliser le M√äME scaler qu'√† l'entra√Ænement !
   - scaler.fit() ‚Üí √Ä l'entra√Ænement (calcule mean/std)
   - scaler.transform() ‚Üí En production (applique mean/std)
   
   Ne JAMAIS appeler fit() en production !

6. VALIDATION DES INPUTS UTILISATEUR
   -----------------------------------
   JAMAIS faire confiance aux entr√©es utilisateur.
   
   validate_input() v√©rifie les ranges AVANT pr√©diction.
   
   Bonnes pratiques :
   ‚úÖ Valider c√¥t√© client (Streamlit sliders avec min/max)
   ‚úÖ Valider c√¥t√© serveur (validate_input())
   ‚úÖ Afficher des messages d'erreur clairs

7. COMMANDES UTILES
   -----------------
   # Lancer l'application Streamlit
   streamlit run streamlit_app/app.py
   
   # Avec debug (auto-reload)
   streamlit run streamlit_app/app.py --server.runOnSave true
   
   # Tester une fonction utils
   python -c "from streamlit_app.utils import load_models; print(load_models())"
   
   # Clear cache manuellement
   # Dans l'app : Menu (‚ò∞) > Clear cache

8. ERREURS COURANTES ET SOLUTIONS
   --------------------------------
   ‚ùå "Session state has no attribute X"
   ‚úÖ Initialiser dans app.py : if 'X' not in st.session_state: st.session_state.X = default
   
   ‚ùå "Model file not found"
   ‚úÖ V√©rifier que les mod√®les sont entra√Æn√©s (python scripts/2_train_models.py)
   
   ‚ùå "Scaler expects X features but got Y"
   ‚úÖ V√©rifier que feature_cols a le bon ordre et nombre de colonnes
   
   ‚ùå "LabelEncoder: classes_ not found"
   ‚úÖ V√©rifier que l'encodeur est bien sauvegard√© apr√®s l'entra√Ænement
   
   ‚ùå Page blanche / app ne d√©marre pas
   ‚úÖ V√©rifier les imports (pip install -r requirements.txt)
   
   ‚ùå Cache ne se vide pas
   ‚úÖ Red√©marrer l'app (Ctrl+C puis relancer)

9. GESTION DES S√âQUENCES TEMPORELLES (LSTM)
   -----------------------------------------
   LSTM n√©cessite une s√©quence de SEQUENCE_LENGTH observations (ex: 12 heures).
   
   Format d'entr√©e LSTM : (samples, timesteps, features)
   - samples = 1 (une pr√©diction √† la fois)
   - timesteps = 12 (12 heures d'historique)
   - features = 9 (9 colonnes)
   
   Shape finale : (1, 12, 9)
   
   Construction de la s√©quence :
   1. R√©cup√©rer les 11 derni√®res heures de l'historique
   2. Ajouter l'observation actuelle (1 heure)
   3. Total : 12 heures
   4. Normaliser TOUTE la s√©quence
   5. Reshaper pour LSTM
   
   Si historique insuffisant (<11 heures) :
   ‚Üí LSTM non utilisable, utilise seulement LightGBM

10. PROBABILIT√âS VS PR√âDICTIONS BINAIRES
    -------------------------------------
    Les mod√®les retournent des PROBABILIT√âS (0.0-1.0), pas des 0/1.
    
    Probabilit√© : "Il y a 73% de chance de coupure"
    Pr√©diction : "Oui, coupure" (si proba >= seuil)
    
    Conversion :
    ```python
    proba = 0.73
    seuil = 0.21  # Seuil optimal trouv√© √† l'entra√Ænement
    pred = 1 if proba >= seuil else 0
    ```
    
    Pourquoi afficher les probabilit√©s ?
    - Plus informatif ("73%" > "Oui")
    - Permet √† l'utilisateur de juger
    - Utile pour fixer des seuils personnalis√©s

11. STATUT DE RISQUE (FAIBLE/MOD√âR√â/√âLEV√â)
    ---------------------------------------
    On transforme les probabilit√©s en statuts compr√©hensibles.
    
    Mapping (exemple avec seuils config.py) :
    - [0.0, 0.3[ ‚Üí üü¢ Risque Faible (green)
    - [0.3, 0.7[ ‚Üí üü† Risque Mod√©r√© (orange)
    - [0.7, 1.0] ‚Üí üî¥ Risque √âlev√© (red)
    
    Pourquoi ?
    - Utilisateurs non techniques pr√©f√®rent "Risque √âlev√©" √† "0.82"
    - Couleurs facilitent la lecture (rouge = danger)
    - Emojis augmentent l'attention
    
    Ces seuils sont configurables dans config.py :
    - THRESHOLD_MODERATE = 0.3
    - THRESHOLD_HIGH = 0.7

12. BONNES PRATIQUES - FONCTIONS PURES
    ------------------------------------
    Les fonctions de utils.py sont "pures" quand possible.
    
    Fonction pure :
    - Pas d'effets de bord
    - M√™me input ‚Üí M√™me output (d√©terministe)
    - Pas de modification d'√©tat global
    
    Exemple :
    ```python
    # ‚úÖ PURE
    def format_percentage(value):
        return f"{value * 100:.2f}%"
    
    # ‚ùå IMPURE (modifie une variable globale)
    counter = 0
    def format_percentage(value):
        global counter
        counter += 1  # Effet de bord !
        return f"{value * 100:.2f}%"
    ```
    
    Avantages des fonctions pures :
    - Testables facilement
    - Pas de surprises
    - Parall√©lisables
    - Cachables (memoization)

13. STRUCTURE D'UN BON FICHIER UTILS
    ----------------------------------
    Organisation logique par SECTIONS :
    
    1. Imports
    2. Chargement des ressources (mod√®les, BD)
    3. Logique m√©tier principale (pr√©dictions)
    4. Acc√®s aux donn√©es
    5. Affichage et validation
    6. Utilitaires divers
    
    Chaque section = une responsabilit√©.

14. DEBUGGING STREAMLIT
    --------------------
    Outils utiles :
    
    # Afficher des variables en debug
    st.write("Debug:", variable)
    
    # Afficher un DataFrame
    st.dataframe(df)
    
    # Afficher un objet JSON
    st.json(dict_object)
    
    # Logs dans la console
    print(f"‚ö†Ô∏è Debug: {variable}")
    
    # Exception avec stack trace
    st.exception(exception_object)
    
    # Progress bar pour op√©rations longues
    with st.spinner('Calcul en cours...'):
        result = long_operation()

15. OPTIMISATIONS POSSIBLES
    ------------------------
    Ce fichier est d√©j√† optimis√©, mais possibilit√©s d'am√©lioration :
    
    - Batch predictions (pr√©dire plusieurs quartiers en une fois)
    - Async DB queries (requ√™tes parall√®les)
    - Compression des mod√®les (quantization)
    - CDN pour assets statiques
    - Redis pour cache distribu√©
    - Monitoring (Prometheus, Grafana)
    - A/B testing (tester diff√©rents seuils)

16. R√âCAPITULATIF DES FONCTIONS
    ----------------------------
    CHARGEMENT :
    - load_models() : Charge LightGBM, LSTM, scaler, encodeur (avec cache)
    - get_database() : Initialise la connexion BD (avec cache)
    
    PR√âDICTIONS :
    - make_prediction_single() : Fait une pr√©diction compl√®te (LightGBM + LSTM)
    
    DONN√âES :
    - get_historical_data() : R√©cup√®re l'historique depuis la BD
    - get_statistics_by_quartier() : Calcule les stats agr√©g√©es
    
    AFFICHAGE :
    - format_percentage() : Formate 0.07 ‚Üí "7.00%"
    - get_risk_color() : Probabilit√© ‚Üí "green/orange/red"
    - display_metric_card() : Affiche une m√©trique Streamlit
    - validate_input() : Valide les entr√©es utilisateur
    
    UTILITAIRES :
    - get_quartier_coords() : Retourne les coordonn√©es GPS
    - get_quartier_list() : Liste des noms de quartiers
    - save_prediction_to_db() : Sauvegarde une pr√©diction

17. WORKFLOW TYPIQUE D'UNE PR√âDICTION
    -----------------------------------
    1. Utilisateur saisit : temp√©rature, humidit√©, vent, consommation, quartier
    2. validate_input() v√©rifie les ranges
    3. make_prediction_single() est appel√©e :
       a. Cr√©ation du DataFrame
       b. G√©n√©ration des features temporelles
       c. Encodage du quartier
       d. Normalisation (StandardScaler)
       e. Pr√©diction LightGBM
       f. Pr√©diction LSTM (si historique disponible)
       g. Calcul de la probabilit√© moyenne
       h. D√©termination du statut de risque
    4. R√©sultats affich√©s dans l'interface Streamlit
    5. (Optionnel) save_prediction_to_db() sauvegarde dans la BD

18. D√âPENDANCES CRITIQUES
    ----------------------
    Ce fichier d√©pend de :
    
    MODULES INTERNES :
    - src/config.py : Constantes (chemins, seuils, coordonn√©es)
    - src/data_pipeline.py : DataPipeline.create_time_features()
    - src/database.py : DatabaseManager
    
    BIBLIOTH√àQUES EXTERNES :
    - streamlit : Framework d'interface
    - pandas : Manipulation de donn√©es
    - numpy : Calculs num√©riques
    - joblib : Chargement mod√®les/scaler/encodeur
    - tensorflow : Chargement LSTM
    
    FICHIERS REQUIS (g√©n√©r√©s par l'entra√Ænement) :
    - models/lgbm_model.joblib : Mod√®le LightGBM
    - models/lstm_model.h5 : Mod√®le LSTM
    - models/lstm_threshold.txt : Seuil LSTM
    - models/scaler.joblib : StandardScaler
    - models/encoders.joblib : LabelEncoder
    
    Si un fichier manque ‚Üí Erreur au chargement ‚Üí Affichage dans Streamlit

19. TESTS UNITAIRES POSSIBLES
    --------------------------
    Exemples de tests √† √©crire pour ce module :
    
    ```python
    def test_format_percentage():
        assert format_percentage(0.07) == "7.00%"
        assert format_percentage(1.0) == "100.00%"
    
    def test_validate_input():
        # Valid
        is_valid, _ = validate_input(25, 60, 10, 500)
        assert is_valid == True
        
        # Invalid temperature
        is_valid, msg = validate_input(50, 60, 10, 500)
        assert is_valid == False
        assert "Temp√©rature" in msg
    
    def test_get_risk_color():
        assert get_risk_color(0.1) == "green"
        assert get_risk_color(0.5) == "orange"
        assert get_risk_color(0.9) == "red"
    
    def test_get_quartier_list():
        quartiers = get_quartier_list()
        assert len(quartiers) > 0
        assert "Dakar-Plateau" in quartiers
    ```

20. MAINTENANCE ET √âVOLUTION
    -------------------------
    Ce fichier est stable mais peut √©voluer :
    
    AJOUTS FUTURS POSSIBLES :
    - Nouvelles m√©triques d'affichage
    - Support de nouveaux mod√®les (XGBoost, Random Forest)
    - Pr√©dictions batch (plusieurs quartiers simultan√©ment)
    - Export des r√©sultats (PDF, Excel)
    - Notifications par email/SMS
    - Int√©gration API externe (m√©t√©o en temps r√©el)
    
    R√àGLES DE MAINTENANCE :
    - Une fonction = une responsabilit√© (Single Responsibility Principle)
    - Toujours documenter les nouvelles fonctions
    - Ajouter des tests unitaires
    - Maintenir la coh√©rence du style de code
    - Versionner les changements (Git)
    
    SIGNAUX D'ALERTE :
    - Fonction > 50 lignes ‚Üí D√©composer
    - Duplication de code ‚Üí Factoriser
    - Trop de try/except imbriqu√©s ‚Üí Simplifier
    - Import circulaire ‚Üí Revoir l'architecture
"""
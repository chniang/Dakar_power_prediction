# Fichier : src/data_pipeline.py
# Pipeline de prÃ©traitement des donnÃ©es avec SPLIT CHRONOLOGIQUE SIMPLE
# =======================================================================
#
# Ce fichier transforme les donnÃ©es brutes (raw_data.csv) en donnÃ©es prÃªtes
# pour le Machine Learning. C'est l'Ã©tape CRUCIALE entre les donnÃ©es et le modÃ¨le.
#
# Pourquoi un pipeline ? Pour garantir que les mÃªmes transformations sont appliquÃ©es
# pendant l'entraÃ®nement ET pendant la prÃ©diction (cohÃ©rence).
#
# Ã‰tapes du pipeline :
# 1. Charger les donnÃ©es brutes
# 2. Tri chronologique (IMPORTANT : pas par quartier !)
# 3. Feature engineering (crÃ©er heure, jour, mois, is_peak_hour)
# 4. Encodage des quartiers (texte â†’ nombres)
# 5. Split train/test chronologique (80/20)
# 6. Normalisation (StandardScaler sur train, puis appliquÃ© sur test)
#
# âš ï¸ CORRECTION V6 : Suppression de la stratification par quartier qui cassait
# l'ordre temporel et causait l'inversion des prÃ©dictions.

import pandas as pd
import numpy as np
import joblib
from pathlib import Path
from sklearn.preprocessing import StandardScaler, LabelEncoder
import sys

# Import de la configuration
try:
    from src.config import (
        RAW_DATA_FILE, PROCESSED_DATA_FILE,
        SCALER_FILE, ENCODERS_FILE,
        FEATURE_COLUMNS, FEATURES_TO_SCALE, TARGET_COLUMN,
        TEST_SIZE, RANDOM_STATE, SEQUENCE_LENGTH,
        MESSAGES
    )
except ImportError:
    # Si exÃ©cutÃ© depuis un autre rÃ©pertoire
    sys.path.append(str(Path(__file__).parent.parent))
    from src.config import (
        RAW_DATA_FILE, PROCESSED_DATA_FILE,
        SCALER_FILE, ENCODERS_FILE,
        FEATURE_COLUMNS, FEATURES_TO_SCALE, TARGET_COLUMN,
        TEST_SIZE, RANDOM_STATE, SEQUENCE_LENGTH,
        MESSAGES
    )


class DataPipeline:
    """
    Pipeline de prÃ©traitement des donnÃ©es pour Dakar Power Prediction.
    
    Cette classe orchestre toutes les transformations nÃ©cessaires pour passer
    des donnÃ©es brutes aux features normalisÃ©es utilisables par les modÃ¨les ML.
    
    ResponsabilitÃ©s :
    - Chargement des donnÃ©es
    - Feature engineering (crÃ©ation de nouvelles variables)
    - Encodage des variables catÃ©gorielles (quartier â†’ nombre)
    - Normalisation (StandardScaler)
    - Split train/test chronologique
    - CrÃ©ation de sÃ©quences pour LSTM
    
    Exemple d'utilisation :
        pipeline = DataPipeline()
        data = pipeline.process_for_training(save_processed=True)
        X_train, y_train = data['X_train'], data['y_train']
    """
    
    def __init__(self):
        """
        Initialise le pipeline avec les transformers vides.
        
        Les transformers (scaler, label_encoder) seront crÃ©Ã©s lors du fit
        sur les donnÃ©es d'entraÃ®nement, puis rÃ©utilisÃ©s pour le test et la prÃ©diction.
        """
        self.scaler = None              # StandardScaler (normalisation)
        self.label_encoder = None       # LabelEncoder (quartier â†’ 0-5)
        self.feature_columns = FEATURE_COLUMNS  # Les 9 features du modÃ¨le
        self.features_to_scale = FEATURES_TO_SCALE  # Colonnes Ã  normaliser
        self.target_column = TARGET_COLUMN  # 'coupure' (0 ou 1)
        
    def load_raw_data(self, file_path=None):
        """
        Charge les donnÃ©es brutes depuis le CSV.
        
        Args:
            file_path (Path): Chemin vers raw_data.csv (dÃ©faut: config.RAW_DATA_FILE)
            
        Returns:
            pd.DataFrame: DonnÃ©es brutes avec 52,704 lignes Ã— 8 colonnes
        
        Raises:
            FileNotFoundError: Si le fichier n'existe pas
        
        Note:
            parse_dates=['date_heure'] convertit automatiquement la colonne
            'date_heure' en type datetime (au lieu de string).
        """
        if file_path is None:
            file_path = RAW_DATA_FILE
            
        print(f"ğŸ“‚ Chargement des donnÃ©es : {file_path}")
        
        # VÃ©rifier l'existence du fichier
        if not file_path.exists():
            raise FileNotFoundError(f"Fichier non trouvÃ© : {file_path}")
        
        # Charger avec parsing automatique des dates
        df = pd.read_csv(file_path, parse_dates=['date_heure'])
        print(f"   âœ… {len(df):,} enregistrements chargÃ©s")
        
        return df
    
    def create_time_features(self, df):
        """
        CrÃ©e les features temporelles Ã  partir de la date.
        
        Args:
            df (pd.DataFrame): DataFrame contenant 'date_heure'
            
        Returns:
            pd.DataFrame: DataFrame enrichi avec 4 nouvelles colonnes
        
        Features crÃ©Ã©es :
            - heure : 0-23 (heure de la journÃ©e)
            - jour_semaine : 0-6 (0=Lundi, 6=Dimanche)
            - mois : 1-12 (mois de l'annÃ©e)
            - is_peak_hour : 0/1 (heure de pointe ou non)
        
        Logique is_peak_hour :
            Les heures de pointe sont dÃ©finies comme :
            - SoirÃ©e/nuit : 18h-6h (forte consommation rÃ©sidentielle)
            - Matin : 6h-8h (pic matinal)
            
            Pourquoi ? C'est pendant ces heures que le rÃ©seau est le plus sollicitÃ©,
            donc le risque de coupure est plus Ã©levÃ©.
        """
        print("ğŸ• CrÃ©ation des features temporelles...")
        
        df = df.copy()  # Ã‰viter de modifier le DataFrame original
        
        # Extraction des composantes temporelles
        # dt.hour, dt.dayofweek, dt.month sont des accesseurs pandas pour datetime
        df['heure'] = df['date_heure'].dt.hour
        df['jour_semaine'] = df['date_heure'].dt.dayofweek  # 0=Lundi, 6=Dimanche
        df['mois'] = df['date_heure'].dt.month
        
        # Feature binaire : heure de pointe
        # Logique : (18h â‰¤ heure â‰¤ 23h) OU (0h â‰¤ heure â‰¤ 6h) OU (6h â‰¤ heure â‰¤ 8h)
        # OpÃ©rateur | = OU logique (vectorisÃ© sur tout le DataFrame)
        df['is_peak_hour'] = (
            ((df['heure'] >= 18) | (df['heure'] <= 6)) |  # SoirÃ©e/nuit
            (df['heure'].between(6, 8, inclusive='both'))   # Matin
        ).astype(int)  # Convertir bool â†’ int (True=1, False=0)
        
        print(f"   âœ… Features temporelles crÃ©Ã©es")
        
        return df
    
    def encode_categorical(self, df, fit=True):
        """
        Encode les variables catÃ©gorielles (quartier) en nombres.
        
        Args:
            df (pd.DataFrame): DataFrame avec colonne 'quartier'
            fit (bool): Si True, crÃ©e et fit l'encodeur. Si False, utilise l'existant
            
        Returns:
            pd.DataFrame: DataFrame avec colonne 'quartier_encoded' ajoutÃ©e
        
        Encodage :
            LabelEncoder transforme les noms de quartiers en nombres :
            'Dakar-Plateau' â†’ 0
            'Guediawaye' â†’ 1
            'Mermoz-SacrÃ©-Coeur' â†’ 2
            etc.
        
        Pourquoi encoder ?
            Les algorithmes ML ne peuvent pas traiter directement du texte.
            Il faut convertir en nombres.
        
        âš ï¸ Important : 
            - En mode training (fit=True) : On crÃ©e l'encodeur et on le sauvegarde
            - En mode prediction (fit=False) : On charge l'encodeur existant
            
            Pourquoi ? Pour garantir que 'Guediawaye' sera toujours encodÃ© en 1,
            mÃªme dans de nouvelles donnÃ©es.
        """
        print("ğŸ·ï¸ Encodage des variables catÃ©gorielles...")
        
        df = df.copy()
        
        if fit:
            # MODE TRAINING : CrÃ©er et fitter l'encodeur
            self.label_encoder = LabelEncoder()
            df['quartier_encoded'] = self.label_encoder.fit_transform(df['quartier'])
            
            # Sauvegarder pour rÃ©utilisation ultÃ©rieure
            ENCODERS_FILE.parent.mkdir(parents=True, exist_ok=True)
            joblib.dump({'quartier': self.label_encoder}, ENCODERS_FILE)
            print(f"   ğŸ’¾ Encodeur sauvegardÃ© : {ENCODERS_FILE}")
            
        else:
            # MODE PREDICTION : Charger l'encodeur existant
            if not ENCODERS_FILE.exists():
                raise FileNotFoundError(f"Encodeur non trouvÃ© : {ENCODERS_FILE}")
            
            encoders = joblib.load(ENCODERS_FILE)
            self.label_encoder = encoders['quartier']
            
            # Encoder avec gestion des valeurs inconnues
            # Si un quartier n'Ã©tait pas dans le training, on met -1
            df['quartier_encoded'] = df['quartier'].apply(
                lambda x: self.label_encoder.transform([x])[0] 
                if x in self.label_encoder.classes_ 
                else -1  # Valeur par dÃ©faut pour quartier inconnu
            )
            print(f"   ğŸ“‚ Encodeur chargÃ© : {ENCODERS_FILE}")
        
        print(f"   âœ… Encodage terminÃ©")
        
        return df
    
    def scale_features(self, df, fit=True):
        """
        Normalise les features numÃ©riques avec StandardScaler.
        
        Args:
            df (pd.DataFrame): DataFrame avec features numÃ©riques
            fit (bool): Si True, fit le scaler. Si False, utilise le scaler existant
            
        Returns:
            pd.DataFrame: DataFrame avec features normalisÃ©es
        
        StandardScaler :
            Formule : X_scaled = (X - mean) / std
            
            Exemple :
            temp_celsius = [20, 25, 30]
            mean = 25Â°C, std = 5Â°C
            
            20Â°C â†’ (20-25)/5 = -1.0
            25Â°C â†’ (25-25)/5 =  0.0
            30Â°C â†’ (30-25)/5 = +1.0
        
        Pourquoi normaliser ?
            1. Ã‰viter que certaines features dominent (ex: conso_megawatt en MW
               vs temp_celsius en Â°C â†’ Ã©chelles trÃ¨s diffÃ©rentes)
            2. AccÃ©lÃ©rer la convergence des algorithmes ML
            3. OBLIGATOIRE pour LSTM (stabilitÃ© de l'entraÃ®nement)
        
        Features normalisÃ©es (config.FEATURES_TO_SCALE) :
            - temp_celsius : 15-40Â°C
            - vitesse_vent : 0-50 km/h
            - conso_megawatt : 400-1200 MW
        
        Features NON normalisÃ©es :
            - heure (0-23), jour_semaine (0-6), mois (1-12) â†’ Pas besoin
            - humidite_percent â†’ RetirÃ© car causait des problÃ¨mes de corrÃ©lation
        """
        print("ğŸ“Š Normalisation des features...")
        
        df = df.copy()
        
        # SÃ©lectionner uniquement les colonnes qui existent dans le DataFrame
        cols_to_scale = [col for col in self.features_to_scale if col in df.columns]
        
        if fit:
            # MODE TRAINING : CrÃ©er et fitter le scaler
            self.scaler = StandardScaler()
            # fit_transform() calcule mean et std, puis normalise
            df[cols_to_scale] = self.scaler.fit_transform(df[cols_to_scale])
            
            # Sauvegarder le scaler (avec mean et std mÃ©morisÃ©s)
            SCALER_FILE.parent.mkdir(parents=True, exist_ok=True)
            joblib.dump(self.scaler, SCALER_FILE)
            print(f"   ğŸ’¾ Scaler sauvegardÃ© : {SCALER_FILE}")
            
        else:
            # MODE PREDICTION : Charger le scaler existant
            if not SCALER_FILE.exists():
                raise FileNotFoundError(f"Scaler non trouvÃ© : {SCALER_FILE}")
            
            self.scaler = joblib.load(SCALER_FILE)
            # transform() utilise les mean et std du training (pas de fit !)
            df[cols_to_scale] = self.scaler.transform(df[cols_to_scale])
            print(f"   ğŸ“‚ Scaler chargÃ© : {SCALER_FILE}")
        
        print(f"   âœ… Normalisation terminÃ©e")
        
        return df
    
    def prepare_features(self, df, include_target=True):
        """
        PrÃ©pare les features finales pour le modÃ¨le.
        
        Args:
            df (pd.DataFrame): DataFrame prÃ©traitÃ©
            include_target (bool): Si True, retourne (X, y). Si False, retourne X uniquement
            
        Returns:
            tuple: (X, y) si include_target=True, sinon X uniquement
        
        Cette fonction sÃ©lectionne uniquement les 9 colonnes nÃ©cessaires au modÃ¨le
        (dÃ©finies dans config.FEATURE_COLUMNS) et sÃ©pare X (features) de y (target).
        
        Ordre des features (IMPORTANT) :
            1. temp_celsius
            2. humidite_percent
            3. vitesse_vent
            4. conso_megawatt
            5. heure
            6. jour_semaine
            7. mois
            8. is_peak_hour
            9. quartier_encoded
        """
        print("ğŸ¯ PrÃ©paration des features finales...")
        
        # SÃ©lectionner uniquement les features qui existent
        available_features = [col for col in self.feature_columns if col in df.columns]
        X = df[available_features].copy()
        
        print(f"   âœ… {len(available_features)} features sÃ©lectionnÃ©es")
        
        if include_target:
            # Mode training : Retourner X et y
            if self.target_column not in df.columns:
                raise ValueError(f"Colonne cible '{self.target_column}' non trouvÃ©e")
            
            y = df[self.target_column].copy()
            return X, y
        
        # Mode prediction : Retourner uniquement X
        return X
    
    def create_sequences(self, X, y=None, sequence_length=SEQUENCE_LENGTH):
        """
        CrÃ©e des sÃ©quences temporelles pour le LSTM.
        
        Args:
            X (np.ndarray): Features (n_samples, n_features)
            y (np.ndarray): Target (optionnel)
            sequence_length (int): Longueur des sÃ©quences (dÃ©faut: 12 heures)
            
        Returns:
            tuple: (X_seq, y_seq) si y fourni, sinon X_seq uniquement
        
        Principe :
            Le LSTM a besoin d'historique pour prÃ©dire. On crÃ©e des fenÃªtres
            glissantes de 12 heures.
        
        Exemple avec sequence_length=3 :
            X = [[x1], [x2], [x3], [x4], [x5]]
            y = [y1, y2, y3, y4, y5]
            
            SÃ©quences crÃ©Ã©es :
            X_seq[0] = [x1, x2, x3]  â†’  y_seq[0] = y4 (prÃ©dire heure 4 avec 1-2-3)
            X_seq[1] = [x2, x3, x4]  â†’  y_seq[1] = y5 (prÃ©dire heure 5 avec 2-3-4)
        
        Shape finale :
            X : (n_samples, n_features) = (52704, 9)
            X_seq : (n_samples - 12, 12, 9) = (52692, 12, 9)
            
            Explication : On perd 12 Ã©chantillons car on ne peut pas crÃ©er
            de sÃ©quence pour les 12 premiÃ¨res heures (pas d'historique).
        
        âš ï¸ Correction V6 :
            Conversion en numpy array AVANT la boucle pour Ã©viter les problÃ¨mes
            d'indexation avec pandas Series.
        """
        print(f"ğŸ”„ CrÃ©ation des sÃ©quences (longueur={sequence_length})...")
        
        # Convertir en numpy array pour Ã©viter les problÃ¨mes d'index pandas
        # Si X est dÃ©jÃ  un ndarray, on ne fait rien
        X = np.array(X) if not isinstance(X, np.ndarray) else X
        if y is not None:
            y = np.array(y) if not isinstance(y, np.ndarray) else y
        
        X_seq = []
        y_seq = []
        
        # Boucle sur les Ã©chantillons (Ã  partir de l'index sequence_length)
        for i in range(sequence_length, len(X)):
            # CrÃ©er une sÃ©quence : 12 heures prÃ©cÃ©dentes
            X_seq.append(X[i-sequence_length:i])
            
            # Target : heure actuelle
            if y is not None:
                y_seq.append(y[i])
        
        # Convertir les listes en arrays numpy
        X_seq = np.array(X_seq)
        
        print(f"   âœ… {len(X_seq):,} sÃ©quences crÃ©Ã©es")
        
        if y is not None:
            y_seq = np.array(y_seq)
            return X_seq, y_seq
        
        return X_seq
    
    def split_data_chronological_stratified(self, X, y, test_size=TEST_SIZE):
        """
        âœ… CORRECTION V6 : Split chronologique SIMPLE (80/20)
        
        Pourquoi "chronologique simple" ?
            Avant, j'avais un split stratifiÃ© par quartier qui cassait l'ordre
            temporel et causait l'inversion des prÃ©dictions.
        
        Maintenant :
            - On trie les donnÃ©es par date UNIQUEMENT (pas par quartier)
            - On prend les 80% premiers pour train
            - On prend les 20% derniers pour test
        
        Args:
            X: Features (DataFrame ou ndarray)
            y: Target (Series ou ndarray)
            test_size (float): Proportion du test set (dÃ©faut: 0.2)
            
        Returns:
            tuple: (X_train, X_test, y_train, y_test)
        
        Exemple :
            52,704 enregistrements
            Split Ã  l'index 42,163 (80%)
            
            Train : indices 0 Ã  42,162 (Janvier-Octobre 2024)
            Test : indices 42,163 Ã  52,703 (Novembre-DÃ©cembre 2024)
        
        âš ï¸ CRITIQUE : Pourquoi pas de split alÃ©atoire ?
            Split alÃ©atoire â†’ FUITE DE DONNÃ‰ES !
            Le modÃ¨le verrait des donnÃ©es futures pendant le training.
            
            Exemple :
            Train : [Janvier, Mars, Mai, Juillet]
            Test : [FÃ©vrier, Avril, Juin, AoÃ»t]
            â†’ Le modÃ¨le a vu Mars pendant training, puis doit prÃ©dire FÃ©vrier (le passÃ©) !
        
        âš ï¸ Pourquoi pas de stratification par quartier ?
            Stratifier par quartier â†’ CASSE L'ORDRE TEMPOREL !
            On mÃ©langerait les dates pour garantir 80/20 par quartier.
            â†’ Causerait l'inversion des prÃ©dictions (problÃ¨me rÃ©solu en V6).
        """
        print(f"âœ‚ï¸ SÃ©paration CHRONOLOGIQUE SIMPLE ({int((1-test_size)*100)}%/{int(test_size*100)})...")
        
        # Calculer l'index de sÃ©paration
        # int() arrondit Ã  l'entier infÃ©rieur
        split_idx = int(len(X) * (1 - test_size))
        
        # Split simple selon le type (DataFrame ou array)
        if isinstance(X, pd.DataFrame):
            # iloc = sÃ©lection par position (0 to split_idx)
            X_train = X.iloc[:split_idx]
            X_test = X.iloc[split_idx:]
        else:
            # Slicing numpy
            X_train = X[:split_idx]
            X_test = X[split_idx:]
        
        # Target (fonctionne pour Series et array)
        y_train = y[:split_idx]
        y_test = y[split_idx:]
        
        # Affichage des statistiques
        print(f"   âœ… Train: {len(X_train):,} ({len(X_train)/len(X)*100:.1f}%)")
        print(f"   âœ… Test:  {len(X_test):,} ({len(X_test)/len(X)*100:.1f}%)")
        print(f"   ğŸ“… Split chronologique simple (pas de stratification)")
        
        # Afficher les taux de coupure (vÃ©rifier qu'ils sont similaires)
        # Si trop diffÃ©rents â†’ ProblÃ¨me de reprÃ©sentativitÃ©
        train_rate = y_train.mean() * 100
        test_rate = y_test.mean() * 100
        print(f"   ğŸ“Š Train - Taux coupure: {train_rate:.2f}%")
        print(f"   ğŸ“Š Test  - Taux coupure: {test_rate:.2f}%")
        
        return X_train, X_test, y_train, y_test
    
    def process_for_training(self, save_processed=True):
        """
        Pipeline complet pour l'entraÃ®nement des modÃ¨les.
        
        Args:
            save_processed (bool): Si True, sauvegarde processed_data.csv
            
        Returns:
            dict: Dictionnaire contenant toutes les donnÃ©es prÃ©parÃ©es
            {
                'X_train': ndarray normalisÃ©,
                'X_test': ndarray normalisÃ©,
                'y_train': ndarray,
                'y_test': ndarray,
                'feature_names': liste des noms de features,
                'scaler': StandardScaler fittÃ©,
                'label_encoder': LabelEncoder fittÃ©
            }
        
        Ce dictionnaire contient tout ce qui est nÃ©cessaire pour :
        1. EntraÃ®ner les modÃ¨les (X_train, y_train)
        2. Ã‰valuer les modÃ¨les (X_test, y_test)
        3. Faire des prÃ©dictions futures (scaler, label_encoder)
        
        Pipeline en 7 Ã©tapes :
            1. Charger raw_data.csv
            2. âœ… Trier par date UNIQUEMENT (correction V6)
            3. Feature engineering (heure, jour, mois, is_peak_hour)
            4. Encoder quartiers (texte â†’ nombres)
            5. Split 80/20 chronologique
            6. Normaliser (fit sur train, transform sur test)
            7. Retourner tout dans un dict
        """
        print("\n" + "="*60)
        print("ğŸ”„ PIPELINE DE PRÃ‰TRAITEMENT V6 - MODE TRAINING")
        print("="*60 + "\n")
        
        # --- Ã‰tape 1 : Chargement ---
        df = self.load_raw_data()
        
        # --- Ã‰tape 2 : âœ… CORRECTION V6 - Tri chronologique simple ---
        # AVANT (V5 et antÃ©rieurs) : df.sort_values(['quartier', 'date_heure'])
        # â†’ Triait par quartier d'abord, ce qui cassait l'ordre temporel global
        # â†’ Causait l'inversion des prÃ©dictions
        #
        # APRÃˆS (V6) : df.sort_values('date_heure')
        # â†’ Trie uniquement par date, ordre chronologique pur
        print("ğŸ“… Tri chronologique (par date uniquement)...")
        df = df.sort_values('date_heure').reset_index(drop=True)
        print(f"   âœ… DonnÃ©es triÃ©es de {df['date_heure'].min()} Ã  {df['date_heure'].max()}")
        
        # --- Ã‰tape 3 : Feature engineering ---
        df = self.create_time_features(df)
        
        # --- Ã‰tape 4 : Encodage ---
        df = self.encode_categorical(df, fit=True)
        
        # --- Ã‰tape 5 : Sauvegarde intermÃ©diaire (avant normalisation) ---
        # Utile pour l'analyse exploratoire des donnÃ©es (EDA)
        if save_processed:
            PROCESSED_DATA_FILE.parent.mkdir(parents=True, exist_ok=True)
            df.to_csv(PROCESSED_DATA_FILE, index=False)
            print(f"ğŸ’¾ DonnÃ©es prÃ©traitÃ©es sauvegardÃ©es : {PROCESSED_DATA_FILE}\n")
        
        # --- Ã‰tape 6 : PrÃ©paration X et y ---
        X, y = self.prepare_features(df, include_target=True)
        
        # --- Ã‰tape 7 : Split train/test ---
        X_train, X_test, y_train, y_test = self.split_data_chronological_stratified(X, y)
        
        # --- Ã‰tape 8 : Normalisation ---
        # CRITIQUE : fit sur train UNIQUEMENT !
        # Si on fit sur train+test â†’ FUITE DE DONNÃ‰ES (le modÃ¨le verrait le futur)
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)  # fit + transform
        X_test_scaled = self.scaler.transform(X_test)        # transform uniquement
        
        # Sauvegarder le scaler pour rÃ©utilisation
        joblib.dump(self.scaler, SCALER_FILE)
        print(f"ğŸ’¾ Scaler sauvegardÃ© : {SCALER_FILE}\n")
        
        print("="*60)
        print("âœ… PRÃ‰TRAITEMENT TERMINÃ‰ (V6)")
        print("="*60 + "\n")
        
        # Retourner tout dans un dictionnaire pratique
        return {
            'X_train': X_train_scaled,
            'X_test': X_test_scaled,
            'y_train': y_train,
            'y_test': y_test,
            'feature_names': X_train.columns.tolist() if hasattr(X_train, 'columns') else self.feature_columns,
            'scaler': self.scaler,
            'label_encoder': self.label_encoder
        }
    
    def process_for_prediction(self, df):
        """
        Pipeline pour la prÃ©diction sur de nouvelles donnÃ©es.
        
        Args:
            df (pd.DataFrame): Nouvelles donnÃ©es (sans colonne 'coupure')
            
        Returns:
            np.ndarray: Features normalisÃ©es prÃªtes pour model.predict()
        
        DiffÃ©rences avec process_for_training :
            - Pas de fit (utilise les transformers sauvegardÃ©s)
            - Pas de split train/test
            - Pas de colonne cible
        
        Utilisation dans Streamlit :
            Quand l'utilisateur bouge les sliders (temp, humiditÃ©, etc.),
            on crÃ©e un DataFrame avec ces valeurs, on applique ce pipeline,
            puis on fait model.predict(X_scaled).
        """
        print("ğŸ”® PrÃ©traitement pour prÃ©diction...")
        
        # 1. Features temporelles
        df = self.create_time_features(df)
        
        # 2. Encoder (utilise l'encodeur existant, fit=False)
        df = self.encode_categorical(df, fit=False)
        
        # 3. PrÃ©parer X (sans y car on ne connaÃ®t pas encore la coupure)
        X = self.prepare_features(df, include_target=False)
        
        # 4. Normaliser (utilise le scaler existant)
        if self.scaler is None:
            # Si pas encore chargÃ©, charger depuis le fichier
            self.scaler = joblib.load(SCALER_FILE)
        
        X_scaled = self.scaler.transform(X)
        
        print(f"   âœ… {len(X_scaled)} Ã©chantillons prÃªts pour prÃ©diction")
        
        return X_scaled


def main():
    """
    Fonction de test du pipeline.
    
    ExÃ©cutÃ©e quand on lance : python src/data_pipeline.py
    
    Permet de vÃ©rifier que le pipeline fonctionne correctement
    et d'afficher les shapes des donnÃ©es gÃ©nÃ©rÃ©es.
    """
    pipeline = DataPipeline()
    
    # Tester le pipeline complet
    data = pipeline.process_for_training(save_processed=True)
    
    # Afficher un rÃ©sumÃ©
    print("\nğŸ“Š RÃ©sumÃ© des donnÃ©es prÃ©parÃ©es :")
    print(f"   â€¢ X_train shape : {data['X_train'].shape}")
    print(f"   â€¢ X_test shape  : {data['X_test'].shape}")
    print(f"   â€¢ y_train shape : {data['y_train'].shape}")
    print(f"   â€¢ y_test shape  : {data['y_test'].shape}")
    print(f"   â€¢ Features      : {len(data['feature_names'])}")
    print(f"\n   Features : {data['feature_names']}")
    
    return data


if __name__ == "__main__":
    main()
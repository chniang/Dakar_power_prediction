# Fichier : scripts/3_evaluate_models.py
# Script d'√©valuation et comparaison des mod√®les
# ===============================================
#
# OBJECTIF PRINCIPAL :
# Ce script compare les performances des 2 mod√®les ML (LightGBM vs LSTM)
# en calculant leurs m√©triques et en g√©n√©rant des visualisations.
#
# FONCTIONNALIT√âS :
# 1. Charge les mod√®les entra√Æn√©s + leurs seuils optimaux
# 2. Fait des pr√©dictions sur le test set
# 3. Aligne les donn√©es (important pour LSTM qui perd SEQUENCE_LENGTH √©chantillons)
# 4. Calcule toutes les m√©triques (Accuracy, Precision, Recall, F1, ROC-AUC)
# 5. G√©n√®re 3 graphiques de comparaison (Confusion Matrix, ROC, Precision-Recall)
# 6. Sauvegarde un rapport texte avec recommandations
#
# DUR√âE : ~30 secondes
#
# UTILISATION :
# python scripts/3_evaluate_models.py                  # Affiche les graphiques
# python scripts/3_evaluate_models.py --no-plots       # Sans graphiques
# python scripts/3_evaluate_models.py --save-plots     # Sauvegarde PNG

import sys
from pathlib import Path
import argparse
import warnings
from datetime import datetime

# Suppression des warnings (√©vite le bruit dans la console)
warnings.filterwarnings('ignore')

# Ajout du r√©pertoire racine au path Python (pour importer src/)
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

# Biblioth√®ques externes
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import tensorflow as tf
from tabulate import tabulate

# M√©triques scikit-learn
from sklearn.metrics import (
    confusion_matrix,           # Matrice TN/FP/FN/TP
    roc_curve, auc,             # Courbe ROC et AUC
    precision_recall_curve,     # Courbe Precision-Recall
    average_precision_score,    # Aire sous PR curve
    accuracy_score,             # % de pr√©dictions correctes
    precision_score,            # % de vrais positifs parmi les pr√©dictions positives
    recall_score,               # % de vrais positifs d√©tect√©s
    f1_score,                   # Moyenne harmonique Precision/Recall
    roc_auc_score               # Aire sous ROC curve
)

# Modules internes du projet
from src.data_pipeline import DataPipeline
from src.config import LGBM_MODEL_FILE, LSTM_MODEL_FILE, SEQUENCE_LENGTH


# ============================================================================
# SECTION 1 : VISUALISATIONS
# ============================================================================

def plot_confusion_matrices(y_true, y_pred_lgbm, y_pred_lstm, save_path=None):
    """
    Affiche les matrices de confusion des 2 mod√®les c√¥te √† c√¥te.
    
    MATRICE DE CONFUSION :
    Une grille 2√ó2 qui montre les 4 types de pr√©dictions possibles :
    
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                ‚îÇ  Pr√©dit: Pas de  ‚îÇ  Pr√©dit: Coupure ‚îÇ
    ‚îÇ                ‚îÇ     Coupure      ‚îÇ                  ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ R√©el: Pas de   ‚îÇ   TN (Vrai N√©g)  ‚îÇ   FP (Faux Pos)  ‚îÇ
    ‚îÇ    Coupure     ‚îÇ   ‚úÖ Correct     ‚îÇ   ‚ùå Fausse      ‚îÇ
    ‚îÇ                ‚îÇ                  ‚îÇ      Alerte      ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ R√©el: Coupure  ‚îÇ   FN (Faux N√©g)  ‚îÇ   TP (Vrai Pos)  ‚îÇ
    ‚îÇ                ‚îÇ   ‚ùå Rat√©        ‚îÇ   ‚úÖ Correct     ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    
    POURQUOI C'EST IMPORTANT :
    - TN/TP (diagonale) : Bonnes pr√©dictions ‚Üí on veut maximiser
    - FP : Fausse alerte ‚Üí G√™nant mais pas grave
    - FN : Coupure rat√©e ‚Üí TR√àS GRAVE (pas de pr√©vention)
    
    Dans notre cas, FN est le pire car ne pas pr√©venir d'une coupure
    a plus d'impact que pr√©voir une coupure qui n'arrive pas.
    
    Args:
        y_true : Vraies √©tiquettes (0=pas de coupure, 1=coupure)
        y_pred_lgbm : Pr√©dictions LightGBM (0 ou 1)
        y_pred_lstm : Pr√©dictions LSTM (0 ou 1)
        save_path : Chemin pour sauvegarder le graphique (None = affichage)
    """
    
    # Cr√©er une figure avec 2 sous-graphiques (1 ligne, 2 colonnes)
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # --- GRAPHIQUE 1 : LightGBM ---
    cm_lgbm = confusion_matrix(y_true, y_pred_lgbm)
    sns.heatmap(
        cm_lgbm,                    # Matrice √† afficher
        annot=True,                 # Afficher les nombres dans les cases
        fmt='d',                    # Format entier (pas de d√©cimales)
        cmap='Blues',               # Palette de couleurs bleues
        ax=axes[0],                 # Premier sous-graphique
        cbar=False,                 # Pas de barre de couleur
        linewidths=.5,              # Lignes fines entre les cases
        linecolor='lightgray'
    )
    axes[0].set_title('LightGBM - Matrice de Confusion', 
                      fontsize=14, fontweight='bold')
    axes[0].set_xlabel('Pr√©diction')
    axes[0].set_ylabel('R√©el')
    axes[0].set_xticklabels(['Pas de coupure', 'Coupure'])
    axes[0].set_yticklabels(['Pas de coupure', 'Coupure'])
    
    # --- GRAPHIQUE 2 : LSTM ---
    cm_lstm = confusion_matrix(y_true, y_pred_lstm)
    sns.heatmap(
        cm_lstm,
        annot=True,
        fmt='d',
        cmap='Oranges',             # Palette orang√©e pour diff√©rencier
        ax=axes[1],                 # Deuxi√®me sous-graphique
        cbar=False,
        linewidths=.5,
        linecolor='lightgray'
    )
    axes[1].set_title('LSTM - Matrice de Confusion', 
                      fontsize=14, fontweight='bold')
    axes[1].set_xlabel('Pr√©diction')
    axes[1].set_ylabel('R√©el')
    axes[1].set_xticklabels(['Pas de coupure', 'Coupure'])
    axes[1].set_yticklabels(['Pas de coupure', 'Coupure'])
    
    # Titre global
    plt.suptitle('Comparaison des Matrices de Confusion', 
                 fontsize=16, fontweight='heavy', y=1.02)
    plt.tight_layout(rect=[0, 0, 1, 0.98])
    
    # Sauvegarde ou affichage
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"   üìä Matrices de confusion sauvegard√©es : {save_path}")
    
    # Afficher si pas de sauvegarde OU si --save-plots ET on veut afficher
    if not save_path or not argparse.ArgumentParser().parse_args().save_plots:
        plt.show()


def plot_roc_curves(y_true, y_proba_lgbm, y_proba_lstm, save_path=None):
    """
    Affiche les courbes ROC (Receiver Operating Characteristic).
    
    COURBE ROC - QU'EST-CE QUE C'EST ?
    C'est un graphique qui montre le compromis entre :
    - TPR (True Positive Rate) = Recall = Sensibilit√©
    - FPR (False Positive Rate) = Taux de fausses alertes
    
    COMMENT √áA MARCHE :
    1. On fait varier le seuil de 0 √† 1
    2. Pour chaque seuil, on calcule TPR et FPR
    3. On trace la courbe TPR vs FPR
    
    INTERPR√âTATION :
    - Courbe proche du coin sup√©rieur gauche = BON (TPR √©lev√©, FPR faible)
    - Courbe diagonale = MAUVAIS (mod√®le al√©atoire)
    - AUC (Aire sous la courbe) r√©sume la performance :
      * AUC = 1.0 : Parfait
      * AUC = 0.9 : Excellent
      * AUC = 0.8 : Tr√®s bon
      * AUC = 0.7 : Bon
      * AUC = 0.5 : Al√©atoire (inutile)
    
    EXEMPLE :
    Si AUC = 0.92, √ßa veut dire qu'il y a 92% de chance que le mod√®le
    classe une coupure r√©elle avec un score plus √©lev√© qu'une non-coupure.
    
    Args:
        y_true : Vraies √©tiquettes
        y_proba_lgbm : Probabilit√©s pr√©dites par LightGBM (0.0 √† 1.0)
        y_proba_lstm : Probabilit√©s pr√©dites par LSTM (0.0 √† 1.0)
        save_path : Chemin pour sauvegarder
    """
    
    # Calculer les courbes ROC pour chaque mod√®le
    fpr_lgbm, tpr_lgbm, _ = roc_curve(y_true, y_proba_lgbm)
    roc_auc_lgbm = auc(fpr_lgbm, tpr_lgbm)
    
    fpr_lstm, tpr_lstm, _ = roc_curve(y_true, y_proba_lstm)
    roc_auc_lstm = auc(fpr_lstm, tpr_lstm)
    
    # Cr√©er le graphique
    plt.figure(figsize=(10, 6))
    
    # Tracer LightGBM
    plt.plot(fpr_lgbm, tpr_lgbm, 
             color='blue', lw=2, 
             label=f'LightGBM (AUC = {roc_auc_lgbm:.3f})')
    
    # Tracer LSTM
    plt.plot(fpr_lstm, tpr_lstm, 
             color='orange', lw=2, 
             label=f'LSTM (AUC = {roc_auc_lstm:.3f})')
    
    # Ligne de r√©f√©rence (mod√®le al√©atoire)
    plt.plot([0, 1], [0, 1], 
             color='gray', lw=1, linestyle='--', 
             label='Al√©atoire (AUC = 0.500)')
    
    # Configuration des axes
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Taux de Faux Positifs (FPR)', fontsize=12)
    plt.ylabel('Taux de Vrais Positifs (TPR)', fontsize=12)
    plt.title('Courbes ROC - Comparaison des Mod√®les', 
              fontsize=14, fontweight='bold')
    plt.legend(loc="lower right", fontsize=11)
    plt.grid(alpha=0.3)
    
    # Sauvegarde ou affichage
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"   üìà Courbes ROC sauvegard√©es : {save_path}")
    
    if not save_path or not argparse.ArgumentParser().parse_args().save_plots:
        plt.show()


def plot_precision_recall_curves(y_true, y_proba_lgbm, y_proba_lstm, save_path=None):
    """
    Affiche les courbes Precision-Recall.
    
    COURBE PRECISION-RECALL - POURQUOI L'UTILISER ?
    Contrairement √† la courbe ROC, la courbe PR est plus informative
    pour les datasets D√âS√âQUILIBR√âS (comme le n√¥tre : 93% classe 0, 7% classe 1).
    
    DIFF√âRENCE AVEC ROC :
    - ROC utilise FPR (faux positifs / tous les n√©gatifs)
    - PR utilise Precision (vrais positifs / tous les pr√©dits positifs)
    
    Avec 93% de classe 0, m√™me beaucoup de FP restent un petit FPR,
    mais affectent √©norm√©ment la Precision. La courbe PR r√©v√®le mieux
    ce probl√®me.
    
    INTERPR√âTATION :
    - Courbe proche du coin sup√©rieur droit = BON
    - AP (Average Precision) r√©sume la performance :
      * AP = 1.0 : Parfait
      * AP > 0.5 : Bon sur donn√©es d√©s√©quilibr√©es
      * AP < baseline : Mauvais
    
    BASELINE :
    C'est la ligne de r√©f√©rence qui repr√©sente un classifieur al√©atoire.
    Baseline = proportion de la classe positive (ici ~7%).
    Un bon mod√®le doit avoir AP >> baseline.
    
    Args:
        y_true : Vraies √©tiquettes
        y_proba_lgbm : Probabilit√©s LightGBM
        y_proba_lstm : Probabilit√©s LSTM
        save_path : Chemin pour sauvegarder
    """
    
    # Calculer les courbes Precision-Recall
    precision_lgbm, recall_lgbm, _ = precision_recall_curve(y_true, y_proba_lgbm)
    ap_lgbm = average_precision_score(y_true, y_proba_lgbm)
    
    precision_lstm, recall_lstm, _ = precision_recall_curve(y_true, y_proba_lstm)
    ap_lstm = average_precision_score(y_true, y_proba_lstm)
    
    # Cr√©er le graphique
    plt.figure(figsize=(10, 6))
    
    # Tracer LightGBM
    plt.plot(recall_lgbm, precision_lgbm, 
             color='blue', lw=2,
             label=f'LightGBM (AP = {ap_lgbm:.3f})')
    
    # Tracer LSTM
    plt.plot(recall_lstm, precision_lstm, 
             color='orange', lw=2,
             label=f'LSTM (AP = {ap_lstm:.3f})')
    
    # Ligne de base (proportion de coupures dans les donn√©es)
    baseline = y_true.sum() / len(y_true)
    plt.axhline(y=baseline, 
                color='gray', linestyle='--', lw=1,
                label=f'Baseline (AP = {baseline:.3f})')
    
    # Configuration des axes
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('Recall (Sensibilit√©)', fontsize=12)
    plt.ylabel('Precision', fontsize=12)
    plt.title('Courbes Precision-Recall - Comparaison des Mod√®les', 
              fontsize=14, fontweight='bold')
    plt.legend(loc="lower left", fontsize=11)
    plt.grid(alpha=0.3)
    
    # Sauvegarde ou affichage
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"   üìà Courbes Precision-Recall sauvegard√©es : {save_path}")
    
    if not save_path or not argparse.ArgumentParser().parse_args().save_plots:
        plt.show()


# ============================================================================
# SECTION 2 : G√âN√âRATION DE RAPPORTS
# ============================================================================

def generate_comparison_table(metrics_lgbm, metrics_lstm):
    """
    G√©n√®re un tableau comparatif des m√©triques.
    
    STRUCTURE DU TABLEAU :
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ M√©trique  ‚îÇ LightGBM ‚îÇ   LSTM   ‚îÇ Diff√©rence ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ Accuracy  ‚îÇ  0.9234  ‚îÇ  0.8945  ‚îÇ  +0.0289   ‚îÇ
    ‚îÇ Precision ‚îÇ  0.8123  ‚îÇ  0.7654  ‚îÇ  +0.0469   ‚îÇ
    ‚îÇ    ...    ‚îÇ   ...    ‚îÇ   ...    ‚îÇ    ...     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    
    COLONNE "DIFF√âRENCE" :
    - Valeur positive ‚Üí LightGBM meilleur
    - Valeur n√©gative ‚Üí LSTM meilleur
    - Permet de voir rapidement les √©carts
    
    Args:
        metrics_lgbm : Dict des m√©triques LightGBM
        metrics_lstm : Dict des m√©triques LSTM
    
    Returns:
        DataFrame avec 3 colonnes : LightGBM, LSTM, Diff√©rence
    """
    
    # Cr√©er le DataFrame √† partir des dictionnaires
    comparison = pd.DataFrame({
        'LightGBM': pd.Series(metrics_lgbm),
        'LSTM': pd.Series(metrics_lstm)
    })
    
    # Calculer la diff√©rence (positif = LightGBM meilleur)
    comparison['Diff√©rence'] = comparison['LightGBM'] - comparison['LSTM']
    
    return comparison


def save_evaluation_report(comparison_df, output_dir):
    """
    Sauvegarde un rapport d'√©valuation complet en fichier texte.
    
    STRUCTURE DU RAPPORT :
    1. En-t√™te avec date/heure
    2. Tableau de comparaison des m√©triques
    3. Analyse d√©taill√©e :
       - Meilleur mod√®le pour chaque m√©trique
       - Interpr√©tation des r√©sultats
    4. Recommandation finale bas√©e sur F1-Score
    
    POURQUOI F1-SCORE POUR LA RECOMMANDATION ?
    Le F1-Score est la m√©trique la plus √©quilibr√©e pour notre cas :
    - Il combine Precision (√©viter fausses alertes) et Recall (d√©tecter coupures)
    - Il p√©nalise les mod√®les d√©s√©quilibr√©s (bon en Precision mais mauvais en Recall)
    - C'est le standard pour les probl√®mes de classification d√©s√©quilibr√©e
    
    Args:
        comparison_df : DataFrame de comparaison (de generate_comparison_table)
        output_dir : Dossier o√π sauvegarder le rapport
    """
    
    # Cr√©er le nom du fichier avec timestamp
    report_path = output_dir / f"evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    
    with open(report_path, 'w', encoding='utf-8') as f:
        # === EN-T√äTE ===
        f.write("="*70 + "\n")
        f.write("RAPPORT D'√âVALUATION ET DE COMPARAISON DES MOD√àLES\n")
        f.write(f"Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*70 + "\n\n")
        
        # === TABLEAU DE COMPARAISON ===
        f.write("COMPARAISON DES M√âTRIQUES\n")
        f.write("-" * 70 + "\n")
        # tabulate transforme le DataFrame en tableau ASCII joliment format√©
        f.write(tabulate(comparison_df.round(6), headers='keys', tablefmt='fancy_grid')) 
        f.write("\n\n")
        
        # === ANALYSE DES PERFORMANCES ===
        f.write("ANALYSE DES PERFORMANCES\n")
        f.write("-" * 70 + "\n")
        
        # D√©terminer le meilleur mod√®le pour chaque m√©trique
        best_accuracy = "LightGBM" if comparison_df.loc['Accuracy', 'LightGBM'] > comparison_df.loc['Accuracy', 'LSTM'] else "LSTM"
        best_precision = "LightGBM" if comparison_df.loc['Precision', 'LightGBM'] > comparison_df.loc['Precision', 'LSTM'] else "LSTM"
        best_recall = "LightGBM" if comparison_df.loc['Recall', 'LightGBM'] > comparison_df.loc['Recall', 'LSTM'] else "LSTM"
        best_f1 = "LightGBM" if comparison_df.loc['F1-Score', 'LightGBM'] > comparison_df.loc['F1-Score', 'LSTM'] else "LSTM"
        best_auc = "LightGBM" if comparison_df.loc['ROC-AUC', 'LightGBM'] > comparison_df.loc['ROC-AUC', 'LSTM'] else "LSTM"
        
        # Afficher le meilleur mod√®le pour chaque m√©trique
        f.write(f"‚Ä¢ Meilleure Accuracy (G√©n√©rale)  : {best_accuracy} ({comparison_df.loc['Accuracy', best_accuracy]:.6f})\n")
        f.write(f"‚Ä¢ Meilleure Precision (Faux Positifs) : {best_precision} ({comparison_df.loc['Precision', best_precision]:.6f})\n")
        f.write(f"‚Ä¢ Meilleur Recall (Vrais Positifs)  : {best_recall} ({comparison_df.loc['Recall', best_recall]:.6f})\n")
        f.write(f"‚Ä¢ Meilleur F1-Score (√âquilibre)  : {best_f1} ({comparison_df.loc['F1-Score', best_f1]:.6f})\n")
        f.write(f"‚Ä¢ Meilleur ROC-AUC (Discrimination)  : {best_auc} ({comparison_df.loc['ROC-AUC', best_auc]:.6f})\n\n")
        
        # === RECOMMANDATION FINALE ===
        f.write("CONCLUSION ET RECOMMANDATION\n")
        f.write("-" * 70 + "\n")
        
        # Recommandation bas√©e sur F1-Score (m√©trique la plus importante)
        if comparison_df.loc['F1-Score', 'LightGBM'] > comparison_df.loc['F1-Score', 'LSTM']:
            f.write("‚úÖ RECOMMANDATION : Utiliser LightGBM comme mod√®le principal.\n")
            f.write("   LightGBM offre un meilleur √©quilibre entre pr√©cision et rappel (F1-Score), ")
            f.write("ce qui est critique pour la d√©tection de coupures dans des donn√©es d√©s√©quilibr√©es.\n")
            f.write(f"   L'√©cart de F1-Score est de {comparison_df.loc['F1-Score', 'Diff√©rence']:.6f} en faveur de LightGBM.\n")
        elif comparison_df.loc['F1-Score', 'LSTM'] > comparison_df.loc['F1-Score', 'LightGBM']:
            f.write("‚úÖ RECOMMANDATION : Utiliser LSTM comme mod√®le principal.\n")
            f.write("   LSTM offre de meilleures performances globales (F1-Score), ")
            f.write("montrant sa capacit√© √† capturer des d√©pendances temporelles pertinentes.\n")
            f.write(f"   L'√©cart de F1-Score est de {abs(comparison_df.loc['F1-Score', 'Diff√©rence']):.6f} en faveur de LSTM.\n")
        else:
             f.write("‚ö†Ô∏è RECOMMANDATION : Les mod√®les ont des performances F1-Score tr√®s similaires. ")
             f.write("Choisir en fonction des contraintes de d√©ploiement (vitesse, simplicit√©, m√©moire).\n")
            
        f.write("\n" + "="*70 + "\n")
    
    print(f"\nüìÑ Rapport d'√©valuation sauvegard√© : {report_path}")


# ============================================================================
# SECTION 3 : FONCTION PRINCIPALE
# ============================================================================

def main():
    """
    Fonction principale du script d'√©valuation.
    
    WORKFLOW COMPLET :
    1. Parser les arguments de ligne de commande
    2. Charger les donn√©es de test (via DataPipeline)
    3. Charger les mod√®les entra√Æn√©s + leurs seuils
    4. Pr√©parer les donn√©es s√©quentielles pour LSTM
    5. Faire les pr√©dictions avec les 2 mod√®les
    6. Aligner les donn√©es (probl√®me : LSTM perd SEQUENCE_LENGTH √©chantillons)
    7. Calculer toutes les m√©triques sur donn√©es align√©es
    8. G√©n√©rer les graphiques de comparaison
    9. Sauvegarder le rapport texte
    
    PROBL√àME D'ALIGNEMENT :
    LightGBM pr√©dit sur tout X_test (N √©chantillons)
    LSTM pr√©dit sur X_test_seq (N - SEQUENCE_LENGTH √©chantillons)
    
    Solution : On coupe les SEQUENCE_LENGTH premi√®res lignes de X_test
    pour avoir la m√™me taille pour les 2 mod√®les.
    """
    
    # === √âTAPE 1 : PARSER LES ARGUMENTS ===
    parser = argparse.ArgumentParser(
        description="√âvalue et compare les mod√®les LightGBM et LSTM"
    )
    parser.add_argument(
        '--no-plots', 
        action='store_true',
        help="Ne pas afficher les graphiques"
    )
    parser.add_argument(
        '--save-plots', 
        action='store_true',
        help="Sauvegarder les graphiques (PNG) dans evaluation_results/"
    )
    
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("üìä SCRIPT 3 : √âVALUATION ET COMPARAISON DES MOD√àLES")
    print("="*70)
    
    # Cr√©er le dossier de sortie pour les r√©sultats
    output_dir = project_root / "evaluation_results"
    output_dir.mkdir(exist_ok=True)
    
    try:
        # === √âTAPE 2 : CHARGER LES DONN√âES ===
        print("\n1Ô∏è‚É£ Chargement des donn√©es de test...")
        pipeline = DataPipeline() 
        
        # Charger les donn√©es pr√©trait√©es + split train/test
        data = pipeline.process_for_training(save_processed=False)
        
        X_test = data['X_test']
        y_test = pd.Series(data['y_test'])  # Convertir en Series pour faciliter l'indexation

        print(f"   ‚úÖ {len(X_test):,} √©chantillons de test charg√©s")
        
        # === √âTAPE 3 : CHARGER LES MOD√àLES ===
        print("\n2Ô∏è‚É£ Chargement des mod√®les...")
        
        # LightGBM (sauvegard√© avec Joblib)
        lgbm_data = joblib.load(LGBM_MODEL_FILE)
        lgbm_model = lgbm_data['model']
        lgbm_threshold = lgbm_data['threshold']
        print(f"   ‚úÖ LightGBM charg√© (seuil = {lgbm_threshold:.3f})")
        
        # LSTM (sauvegard√© avec Keras)
        lstm_model = tf.keras.models.load_model(LSTM_MODEL_FILE)
        lstm_threshold_file = LSTM_MODEL_FILE.parent / "lstm_threshold.txt"
        with open(lstm_threshold_file, 'r') as f:
            lstm_threshold = float(f.read().strip())
        print(f"   ‚úÖ LSTM charg√© (seuil = {lstm_threshold:.3f})")
        
        # === √âTAPE 4 : PR√âPARER LES S√âQUENCES POUR LSTM ===
        print("\n3Ô∏è‚É£ Pr√©paration des donn√©es s√©quentielles pour LSTM...")
        X_test_seq, y_test_seq = pipeline.create_sequences(
            X_test, y_test.values, sequence_length=SEQUENCE_LENGTH
        )
        print(f"   ‚úÖ {len(X_test_seq):,} s√©quences cr√©√©es")
        print(f"   ‚ÑπÔ∏è Perte de {SEQUENCE_LENGTH} √©chantillons (historique)")

        # === √âTAPE 5 : PR√âDICTIONS ===
        print("\n4Ô∏è‚É£ Pr√©dictions des mod√®les...")
        
        # LightGBM : pr√©dit sur tout X_test
        y_proba_lgbm = lgbm_model.predict(X_test)
        y_pred_lgbm = (y_proba_lgbm >= lgbm_threshold).astype(int)
        
        # LSTM : pr√©dit sur X_test_seq (taille r√©duite)
        y_proba_lstm = lstm_model.predict(X_test_seq, verbose=0).flatten()
        y_pred_lstm = (y_proba_lstm >= lstm_threshold).astype(int)
        
        # === √âTAPE 6 : ALIGNEMENT DES DONN√âES ===
        print("\n5Ô∏è‚É£ Alignement des donn√©es pour comparaison √©quitable...")
        
        # PROBL√àME :
        # LightGBM a fait N pr√©dictions (une par ligne de X_test)
        # LSTM a fait N-SEQUENCE_LENGTH pr√©dictions (perd les premi√®res lignes)
        #
        # SOLUTION :
        # On coupe les SEQUENCE_LENGTH premi√®res pr√©dictions de LightGBM
        # pour avoir exactement la m√™me taille que LSTM
        
        start_index = SEQUENCE_LENGTH
        
        # Valeurs r√©elles communes (celles que LSTM peut pr√©dire)
        y_test_common = y_test.values[start_index:]
        
        # Pr√©dictions LightGBM align√©es (on retire les premi√®res)
        y_pred_lgbm_common = y_pred_lgbm[start_index:]
        y_proba_lgbm_common = y_proba_lgbm[start_index:]
        
        # y_test_seq, y_pred_lstm, y_proba_lstm sont d√©j√† align√©s
        # (cr√©√©s par create_sequences)
        
        print(f"   ‚úÖ Taille commune : {len(y_test_common):,} √©chantillons")
        print(f"   ‚ÑπÔ∏è On compare sur les m√™mes donn√©es pour √™tre juste")
        
        # === √âTAPE 7 : CALCUL DES M√âTRIQUES ===
        print("\n6Ô∏è‚É£ Calcul des m√©triques de performance...")
        
        # M√©triques LightGBM (sur donn√©es align√©es)
        metrics_lgbm = {
            'Accuracy': accuracy_score(y_test_common, y_pred_lgbm_common),
            'Precision': precision_score(y_test_common, y_pred_lgbm_common, zero_division=0),
            'Recall': recall_score(y_test_common, y_pred_lgbm_common, zero_division=0),
            'F1-Score': f1_score(y_test_common, y_pred_lgbm_common, zero_division=0),
            'ROC-AUC': roc_auc_score(y_test_common, y_proba_lgbm_common)
        }
        
        # M√©triques LSTM (d√©j√† sur donn√©es align√©es)
        metrics_lstm = {
            'Accuracy': accuracy_score(y_test_seq, y_pred_lstm),
            'Precision': precision_score(y_test_seq, y_pred_lstm, zero_division=0),
            'Recall': recall_score(y_test_seq, y_pred_lstm, zero_division=0),
            'F1-Score': f1_score(y_test_seq, y_pred_lstm, zero_division=0),
            'ROC-AUC': roc_auc_score(y_test_seq, y_proba_lstm)
        }
        
        # Cr√©er le tableau de comparaison
        comparison_df = generate_comparison_table(metrics_lgbm, metrics_lstm)
        
        # === AFFICHAGE DES R√âSULTATS ===
        print("\n" + "="*70)
        print("üìä R√âSULTATS DE LA COMPARAISON DES MOD√àLES")
        print("="*70)
        print(tabulate(comparison_df.round(6), headers='keys', tablefmt='fancy_grid')) 
        
        # === √âTAPE 8 : GRAPHIQUES ===
        if not args.no_plots:
            print("\n7Ô∏è‚É£ G√©n√©ration des graphiques...")
            
            # D√©finir les chemins de sauvegarde (si --save-plots)
            save_cm = output_dir / "confusion_matrices.png" if args.save_plots else None
            save_roc = output_dir / "roc_curves.png" if args.save_plots else None
            save_pr = output_dir / "precision_recall_curves.png" if args.save_plots else None
            
            # G√©n√©rer les 3 graphiques (sur donn√©es align√©es)
            plot_confusion_matrices(y_test_common, y_pred_lgbm_common, y_pred_lstm, save_path=save_cm)
            plot_roc_curves(y_test_common, y_proba_lgbm_common, y_proba_lstm, save_path=save_roc)
            plot_precision_recall_curves(y_test_common, y_proba_lgbm_common, y_proba_lstm, save_path=save_pr)

        # === √âTAPE 9 : RAPPORT TEXTE ===
        print("\n8Ô∏è‚É£ G√©n√©ration du rapport final...")
        save_evaluation_report(comparison_df.round(6), output_dir)
        
        print("\n" + "="*70)
        print("‚úÖ √âVALUATION TERMIN√âE AVEC SUCC√àS")
        print("="*70)
        print(f"\nüìÅ R√©sultats sauvegard√©s dans : {output_dir}")
        
    except Exception as e:
        print(f"\n‚ùå ERREUR LORS DE L'EX√âCUTION : {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


# ============================================================================
# POINT D'ENTR√âE DU SCRIPT
# ============================================================================

if __name__ == "__main__":
    """
    Point d'entr√©e quand on ex√©cute : python scripts/3_evaluate_models.py
    
    EXEMPLES D'UTILISATION :
    
    1. Afficher les graphiques interactifs :
       python scripts/3_evaluate_models.py
    
    2. Sans graphiques (mode rapide) :
       python scripts/3_evaluate_models.py --no-plots
    
    3. Sauvegarder les graphiques en PNG :
       python scripts/3_evaluate_models.py --save-plots
    
    SORTIE DU SCRIPT :
    - evaluation_results/confusion_matrices.png (si --save-plots)
    - evaluation_results/roc_curves.png (si --save-plots)
    - evaluation_results/precision_recall_curves.png (si --save-plots)
    - evaluation_results/evaluation_report_YYYYMMDD_HHMMSS.txt (toujours)
    
    DUR√âE TYPIQUE : ~30 secondes
    """
    main()


# ============================================================================
# NOTES P√âDAGOGIQUES POUR DATA SCIENTIST JUNIOR
# ============================================================================

"""
üìö CONCEPTS CL√âS √Ä RETENIR :

1. M√âTRIQUES D'√âVALUATION
   -------------------------
   ‚Ä¢ Accuracy : % de pr√©dictions correctes (simple mais trompeuse sur donn√©es d√©s√©quilibr√©es)
   ‚Ä¢ Precision : Parmi les pr√©dictions positives, combien sont vraies ? (√©viter fausses alertes)
   ‚Ä¢ Recall : Parmi les cas positifs r√©els, combien sont d√©tect√©s ? (ne rien rater)
   ‚Ä¢ F1-Score : Moyenne harmonique de Precision et Recall (m√©trique d'√©quilibre)
   ‚Ä¢ ROC-AUC : Capacit√© √† discriminer les classes (0.5 = al√©atoire, 1.0 = parfait)

2. PROBL√àME D'ALIGNEMENT DES DONN√âES
   -----------------------------------
   LSTM perd SEQUENCE_LENGTH √©chantillons car il a besoin d'historique.
   
   Exemple avec SEQUENCE_LENGTH=12 :
   - Ligne 0 : Pas assez d'historique (besoin de 12 lignes avant)
   - Ligne 11 : Pas assez d'historique
   - Ligne 12 : OK ! (lignes 0-11 comme historique)
   
   Solution : On compare uniquement sur les lignes 12+ pour les 2 mod√®les.

3. CHOIX DE LA M√âTRIQUE PRINCIPALE
   ---------------------------------
   Pourquoi F1-Score ?
   - Notre dataset est d√©s√©quilibr√© (7% coupures)
   - Accuracy serait trompeuse (un mod√®le qui dit "jamais de coupure" aurait 93% d'accuracy !)
   - F1 p√©nalise les mod√®les qui n√©gligent la classe minoritaire
   - C'est le standard pour classification d√©s√©quilibr√©e

4. INTERPR√âTATION DES GRAPHIQUES
   -------------------------------
   ‚Ä¢ Confusion Matrix : Visualise les 4 types d'erreurs (TN/FP/FN/TP)
   ‚Ä¢ ROC Curve : Trade-off entre Recall et Faux Positifs
   ‚Ä¢ PR Curve : Plus informative sur donn√©es d√©s√©quilibr√©es

5. COMPARAISON LIGHTGBM VS LSTM
   ------------------------------
   Typiquement sur ce projet :
   - LightGBM : Meilleur F1-Score, plus rapide, plus simple
   - LSTM : Capture mieux les d√©pendances temporelles longues, mais plus lourd
   
   LightGBM gagne souvent sur donn√©es tabulaires de taille moyenne (<100k lignes).

6. BONNES PRATIQUES
   -----------------
   ‚úÖ Toujours comparer sur les m√™mes donn√©es (alignement)
   ‚úÖ Utiliser plusieurs m√©triques (pas seulement Accuracy)
   ‚úÖ Visualiser les r√©sultats (graphiques + tableaux)
   ‚úÖ Sauvegarder un rapport texte (tra√ßabilit√©)
   ‚úÖ Tester avec diff√©rents seuils si besoin

7. COMMANDES UTILES
   -----------------
   # √âvaluation compl√®te avec graphiques
   python scripts/3_evaluate_models.py
   
   # Mode rapide sans graphiques
   python scripts/3_evaluate_models.py --no-plots
   
   # Sauvegarder les graphiques
   python scripts/3_evaluate_models.py --save-plots
   
   # Voir les r√©sultats
   cat evaluation_results/evaluation_report_*.txt
"""